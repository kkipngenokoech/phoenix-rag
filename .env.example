# Phoenix RAG Configuration
# Copy this to .env and fill in your values

# LLM Provider: auto, ollama, groq, anthropic, openai
# "auto" = tries Ollama first, falls back to Groq if not available
LLM_PROVIDER=auto

# Model name (depends on provider)
# Ollama: llama3.2, codellama, mistral
# Groq: llama-3.3-70b-versatile, llama-3.1-8b-instant, mixtral-8x7b-32768
# Anthropic: claude-3-5-sonnet-20241022
# OpenAI: gpt-4o, gpt-4o-mini
LLM_MODEL=llama3.2

# API Keys (only needed for cloud providers)
GROQ_API_KEY=your_groq_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# Ollama settings (for local deployment)
OLLAMA_BASE_URL=http://localhost:11434

# Embedding model
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Vector database
CHROMA_PERSIST_DIRECTORY=./data/chroma_db

# Agent settings
MAX_ITERATIONS=10
GROUNDEDNESS_THRESHOLD=0.7
